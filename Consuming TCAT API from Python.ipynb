{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consuming TCAT API from Python\n",
    "\n",
    "So, a student was wondering if Python could be used to ease the burden of moving stuff from [TCAT](https://github.com/digitalmethodsinitiative/dmi-tcat) to Tableau. We provide a TCAT instance and access to the `analysis` interface, where students groups have their own query bins running queries they have designed for their projects.\n",
    "\n",
    "![](https://i.ytimg.com/vi/vRshQcRqEUI/maxresdefault.jpg)\n",
    "\n",
    "Dangernoodle *1* vs. daggermitten *0*\n",
    "\n",
    "## A status quo\n",
    "\n",
    "Currently the students browse to the analysis interface, export a CSV file, and load it up in Tableau, possibly overriding some of the settings, depending on their locale (e.g. a comma `,` is a decimal separator in the Danish locale). No problem, but loads of button pressing for the humans.\n",
    "\n",
    "## A dream\n",
    "\n",
    "Could we re-negotiate the configuration of actors, and translate some of the labour to some software actors, by question autonomy of actors we call 'TCAT', 'Tableau' or 'Student'? Sure thing! Below is a sketch for one of many possible solutions, as a Notebook. We could translate as a cronjob, a microservice, a database trigger or something else instead, or hire an assistant, acquire an intern, or set it up as a Mechanical Turk task, or redesign our workflow to just do the data transport less frequently. Here's one idea for pedagogical and illustrative purposes, as well as for me to learn about the TCAT API and for myself to have a bit of fun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "There are many ways to push data around between TCAT and Tableau, of course. Here, we connect to the not-very-well-known TCAT API, export all the tweets of a query bin, read it into Pandas DataFrame for SQL convenience, store it in a MySQL database, and then connect Tableau to that database.\n",
    "\n",
    "As a bonus, we get the opportunity to poke around the data in Python, since it's brought in. That's what I personally prefer :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for connecting to TCAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import json\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read configuration from [`tcatdb.ini`](tcatdb.ini) which describes to APIs we will use: TCAT and the database. Briefly, it looks like this:\n",
    "\n",
    "    [tcat]\n",
    "    endpoint = http://yourtcatserver/api/querybin.php\n",
    "    user = adminuseraccount\n",
    "    ‚ãÆ\n",
    "    \n",
    "    [db]\n",
    "    engine = mysql\n",
    "    server = databaseserverhost\n",
    "    ‚ãÆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tcatdb.ini']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read(\"tcatdb.ini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = config[\"tcat\"][\"endpoint\"]\n",
    "querybin = \"NavCom2017_08_gang_wars\"\n",
    "controller = \"tweets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uri = \"/\".join([endpoint, querybin, controller])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up HTTP authentication. Our TCAT instance API is exposed only teaching staff, actually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "authinfo = urllib.request.HTTPBasicAuthHandler()\n",
    "authinfo.add_password(realm=config[\"tcat\"][\"realm\"],\n",
    "                      uri=config[\"tcat\"][\"endpoint\"],\n",
    "                      user=config[\"tcat\"][\"user\"],\n",
    "                      passwd=config[\"tcat\"][\"user\"])\n",
    "opener = urllib.request.build_opener(authinfo)\n",
    "opener = urllib.request.build_opener()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above does not actually work as imagined, and I struggled with it. I believe it is because the `urllib.request.HTTPBasicAuthHandler` does not seem to [Base64 encode](https://en.wikipedia.org/wiki/Basic_access_authentication#Client_side) the `%s:%s % (user, password)` string, though I might be mistaken about the issue. Anyway, we will add the `Authorization` HTTP header ourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "creds = urllib.request.base64.encodebytes(bytes(\"%s:%s\" % (config[\"tcat\"][\"user\"], config[\"tcat\"][\"passwd\"]), \"utf-8\")).decode().strip()\n",
    "opener.addheaders.append((\"Authorization\", \"Basic \" + creds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['User-agent', 'Authorization']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[h for h, v in opener.addheaders]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The setup is now complete, let's read data about the query bin, and extract it's contents and store them as a Pandas DataFrame.\n",
    "\n",
    "## Export tweets from TCAT\n",
    "\n",
    "We will make to HTTP requests, the first activates the default `bin-info` action of the API, the second the `tweet-export` action. We might want to date-limit the extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "exportparams = urllib.parse.urlencode({\n",
    "    'action': 'tweet-export',\n",
    "    # 'startdate': '2017-10-05+19:19:00',\n",
    "    # 'enddate': '2017-10-27+13:17:26',\n",
    "    'format': 'csv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bres = opener.open(uri)\n",
    "tres = opener.open(uri + \"?%s\" % exportparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's process the two requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "bininfo = json.loads(bres.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 36)\n"
     ]
    }
   ],
   "source": [
    "if tres.code == 200:\n",
    "    tw = pd.read_csv(tres) # A HTTPResponse is an IO object\n",
    "    print(tw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 18 of 18 tweets.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loaded {len(tw)} of {bininfo['number-selected-tweets']} tweets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>created_at</th>\n",
       "      <th>from_user_name</th>\n",
       "      <th>text</th>\n",
       "      <th>filter_level</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>withheld_copyright</th>\n",
       "      <th>withheld_scope</th>\n",
       "      <th>truncated</th>\n",
       "      <th>...</th>\n",
       "      <th>from_user_utcoffset</th>\n",
       "      <th>from_user_timezone</th>\n",
       "      <th>from_user_lang</th>\n",
       "      <th>from_user_tweetcount</th>\n",
       "      <th>from_user_followercount</th>\n",
       "      <th>from_user_friendcount</th>\n",
       "      <th>from_user_favourites_count</th>\n",
       "      <th>from_user_listed</th>\n",
       "      <th>from_user_withheld_scope</th>\n",
       "      <th>from_user_created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>919121497536557056</td>\n",
       "      <td>1507970616</td>\n",
       "      <td>2017-10-14 08:43:36</td>\n",
       "      <td>AndersEKrag</td>\n",
       "      <td>S√•dan @mkrasnik üòäüëèüòäüëèüòä @weekendavisen #dkpol #b...</td>\n",
       "      <td>low</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>Copenhagen</td>\n",
       "      <td>en</td>\n",
       "      <td>31799</td>\n",
       "      <td>1113</td>\n",
       "      <td>971</td>\n",
       "      <td>14599</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-02-10 06:59:38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows √ó 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id        time           created_at from_user_name  \\\n",
       "5  919121497536557056  1507970616  2017-10-14 08:43:36    AndersEKrag   \n",
       "\n",
       "                                                text filter_level  \\\n",
       "5  S√•dan @mkrasnik üòäüëèüòäüëèüòä @weekendavisen #dkpol #b...          low   \n",
       "\n",
       "   possibly_sensitive  withheld_copyright  withheld_scope  truncated  \\\n",
       "5                 0.0                 NaN             NaN          0   \n",
       "\n",
       "           ...           from_user_utcoffset  from_user_timezone  \\\n",
       "5          ...                        7200.0          Copenhagen   \n",
       "\n",
       "  from_user_lang from_user_tweetcount  from_user_followercount  \\\n",
       "5             en                31799                     1113   \n",
       "\n",
       "   from_user_friendcount from_user_favourites_count from_user_listed  \\\n",
       "5                    971                      14599               45   \n",
       "\n",
       "   from_user_withheld_scope  from_user_created_at  \n",
       "5                       NaN   2010-02-10 06:59:38  \n",
       "\n",
       "[1 rows x 36 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw.sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the TCAT query bin contents in a DataFrame, let's \n",
    "\n",
    "## Set up database connection\n",
    "\n",
    "I am using `mysqlclient` as a driver for SQLAlchemy. We will have set up a database user on the server, allowed it to connect from wherever this Jupyter kernel is running and granted it sufficient permissions, and created an empty database. I did something like this, from MySQL:\n",
    "\n",
    "    CREATE USER 'tcat'@'192.168.1.61' IDENTIFIED BY 'yeahgreatsecrethere';\n",
    "    GRANT ALL PRIVILEGES ON tcat.* TO 'tcat'@'192.168.1.61';\n",
    "\n",
    "Then we prepare a database engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sqlalchemy.create_engine(\"%s://%s:%s@%s/%s\" % (\n",
    "                                  config[\"db\"][\"engine\"],\n",
    "                                  config[\"db\"][\"user\"],\n",
    "                                  config[\"db\"][\"passwd\"],\n",
    "                                  config[\"db\"][\"server\"],\n",
    "                                  config[\"db\"][\"db\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank you Pandas üêº for being powerful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/MySQLdb/cursors.py:318: Warning: (1366, \"Incorrect string value: '\\\\xF0\\\\x9F\\\\x98\\\\x8A\\\\xF0\\\\x9F...' for column 'text' at row 6\")\n",
      "  rows += self.execute(sql + postfix)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/MySQLdb/cursors.py:318: Warning: (1366, \"Incorrect string value: '\\\\xF0\\\\x9F\\\\x98\\\\x8A\\\\xF0\\\\x9F...' for column 'text' at row 7\")\n",
      "  rows += self.execute(sql + postfix)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/MySQLdb/cursors.py:318: Warning: (1366, \"Incorrect string value: '\\\\xF0\\\\x9F\\\\x98\\\\x8A\\\\xF0\\\\x9F...' for column 'text' at row 8\")\n",
      "  rows += self.execute(sql + postfix)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/MySQLdb/cursors.py:318: Warning: (1366, \"Incorrect string value: '\\\\xF0\\\\x9F\\\\x98\\\\x8A\\\\xF0\\\\x9F...' for column 'text' at row 9\")\n",
      "  rows += self.execute(sql + postfix)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/MySQLdb/cursors.py:318: Warning: (1366, \"Incorrect string value: '\\\\xF0\\\\x9F\\\\x98\\\\x8A\\\\xF0\\\\x9F...' for column 'text' at row 10\")\n",
      "  rows += self.execute(sql + postfix)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/MySQLdb/cursors.py:318: Warning: (1366, \"Incorrect string value: '\\\\xF0\\\\x9F\\\\x98\\\\x8A\\\\xF0\\\\x9F...' for column 'text' at row 11\")\n",
      "  rows += self.execute(sql + postfix)\n"
     ]
    }
   ],
   "source": [
    "tw.to_sql(\"tweets\", engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done.\n",
    "\n",
    "## Connect Tableau to the database\n",
    "\n",
    "Next we want to open up Tableau and connect it to the above database.\n",
    "\n",
    "I'm using MySQL database server.\n",
    "\n",
    "![](Tableau connect to a MySQL server.png)\n",
    "\n",
    "![](Tableau define MySQL database connection.png)\n",
    "\n",
    "The server might contain more than one database, possibly something you have generated, so select the one where you stored tweets from TCAT.\n",
    "\n",
    "![](Tableau select database from server.png)\n",
    "\n",
    "The live connection is the default.\n",
    "\n",
    "![](Tableau live connection.png)\n",
    "\n",
    "You are welcome.\n",
    "\n",
    "![](Tableau outcome.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "Many other things could be done, either additionally or instead of what is presented above.\n",
    "\n",
    "Maybe instead of a database, we want to simplify the setup just want to keep overwriting a CSV file, and have Tableau re-read it. While the data is in Python, we might want to do processing of it, or maybe create more database tables. Or we might utilize `ipywidgets` and allow the user to interactively select a subset of tweets to extract. Or we might want to engineer this out of the narrative constrain of a Jupyter Notebook and write it as a \"real\" program. We might want to add robustness and engineer robustness and recovery. TCAT also uses a database for it's own purposes, so of course we could just connect to it directly with Tableau and run SQL directly against it. We might want to question the whole idea of scraping Twitter and navigate ourselves towards more interesting areas of research perhaps, or ask if any of the users have given informed consent to us creeping on them.\n",
    "\n",
    "Or we might want to do something else entirely, like enjoy the day in the park with friends."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
